---
layout: distill
title: Lecture 25
description: Alignment, explainability, and open directions in LLM research
date: 2025-12-08

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Reid Chen
    url: "https://www.deepneural.network"
  - name: Joel Sawatzky
    url: "#"
  - name: Shrivats Sriram
    url: "#"
  - name: Evanie Li
    url: "#"

editors:
  - name: Zefan Cai
    url: "#"
  - name: Wenjie Hu
    url: "#"
  - name: Yufan Cheng
    url: "#"

abstract: >
  This lecture explores the challenges and strategies in making complex machine learning (ML) models explainable and aligned with human values, focusing especially on large language models (LLMs) and their impact.
---

## Phases of Model Training

The training pipeline for modern Large Language Models (LLMs) generally follows a progression from broad pattern matching to specific task alignment.



- **Random Model**: The starting point of the architecture.
- **Pre-training**: The model is trained unsupervised on massive datasets (e.g., Common Crawl) to learn general patterns.
- **Fine-tuning**: The pre-trained model is refined using In-Domain Data via Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF).
- **In-context learning**: During usage, prompts and examples in the input guide the model to produce outputs adapted to user intent without updating weights.

## Why Explainability Matters

Models trained on large data are rarely naturally interpretable to humans. Historically, the field has moved through several phases:
* **2016**: Interpretability is invoked when metrics (like accuracy) are imperfect proxies for the true objective.
* **2017**: Doshi-Velez & Kim defined three modes of evaluation: application-grounded, human-grounded, and functionally-grounded.
* **2017-2020**: Approaches fragmented into Post-Hoc (industry standard), Transparency (niche), and Mechanistic (technically deep).

## Fairness & Sensitive Features

Merely dropping sensitive features like "race" from training data does **not** ensure the model is invariant to them, as biases can be encoded via correlated variables.

**Strategies for Invariance:**
1.  **Remove the feature**: Often insufficient due to correlations.
2.  **Train then clean**: Train on all features, then attempt to remove the learned component associated with the sensitive feature.
3.  **Test-time blinding**: Drop the feature only during inference.
4.  **Modified Loss**: Train with a loss function specifically designed to encourage invariant predictions.

## Interpretability Approaches

### 1. Post-hoc Explanations

These methods attempt to explain a black-box model after it has been trained. While tools like LIME, SHAP, and Integrated Gradients became industry standards, they face significant criticism regarding their fidelity.

* **LIME (Locally Interpretable Model-agnostic Explanations)**: Approximates model behavior with simpler models locally.
  $$
  \xi(x) = \operatorname*{arg\,min}_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g)
  $$
  *Critique*: It is easy to fool LIME and SHAP (Slack et al. 2020).

* **SHAP (SHapley Additive exPlanations)**: Uses Shapley values to assign contribution scores.
  $$
  \phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \cup \{i\}) - f(S)]
  $$

* **Integrated Gradients**: An axiomatic attribution method.
  $$
  IG_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^{1} \frac{\partial F(x' + \alpha(x - x'))}{\partial x_i} d\alpha
  $$

**Cracks in Post-Hoc Explanations:**
* **Insensitivity**: Saliency maps can be insensitive to model weights (Adebayo et al. 2018).
* **Plausibility vs. Faithfulness**: There is a gap between an explanation looking reasonable to a human and it actually reflecting the model's computation (Jacovi & Goldberg 2020).
* **High-Stakes**: Rudin (2019) argues for abandoning post-hoc methods entirely in high-stakes settings.

### 2. Transparency by Design

Instead of explaining a black box, use inherently interpretable models.

- **Generalized Additive Models (GAMs)**: Decompose complex outcomes into a sum of univariate functions: $F(x) = \beta_0 + f_1(x_1) + ... + f_r(x_r)$.
    - *Pros*: Components can be individually visualized.
    - *Cons*: Niche application, often used in healthcare/tabular data rather than unstructured data.
- **Monotonic Nets**: Constrain variables to affect predictions in only one direction.

### 3. Mechanistic Interpretability

- Focuses on circuits, probing, and feature geometry.
- **Reverse Engineering**: Extracting components from trained models.
- **Concept-Based**: Attempting to incorporate human-understandable concepts from the start of training.

### Summary: Interpretability Methods Comparison

| Method | Type | Scope | Adoption |
| :--- | :--- | :--- | :--- |
| **LIME, SHAP, IG** | Post-Hoc | Local/Global | **Industry Standard**: Popular, look convincing, but don't guarantee fidelity. |
| **GAMs, Monotonic Nets** | Transparency | Global | **Niche**: Primarily used in healthcare and tabular settings. |
| **Circuits, Probing** | Mechanistic | Internal | **Research**: Technically deep, rarely user-facing. |

## Scaling Laws vs. Interpretability

"Scale is all you need"? (Kaplan et al. 2021) suggests that language modeling performance improves smoothly according to power laws as we increase three factors:
1.  **Model Size** (Parameters)
2.  **Dataset Size** (Tokens)
3.  **Compute** (PF-days)



Empirical performance follows a power-law relationship: $L(x) = (x/x_0)^{-\alpha}$ provided it is not bottlenecked by the other two factors. However, as models scale, they become less interpretable.

## System Design View of Interpretability

Interpretability is not just about debugging; it is a system design feature. It allows us to move from **Individual Stats** (like a player's points per game) to **System Stats** (like a lineup's net rating), which correlates better with winning.

The three main system design benefits are:
1.  **Information Acquisition**
2.  **Value Alignment**
3.  **Modularity**

### 1. Information Acquisition
*What should we measure?*
Predictive models often treat measurements as fixed, but in biomedicine and other fields, measurement is active and costly. Interpretability helps identify which measurements are actually driving risk.

**Case Study: Severe Maternal Morbidity (SMM)**
Using a GAM to predict SMM revealed that the **BabySize-MaternalHeight Ratio** was the #1 feature for importance—more critical than preeclampsia.
* *Insight*: We just happened to have maternal height data available; interpretability forced the question: "What should we actually be measuring clinically?".



### 2. Value Alignment
*What did the model learn to optimize?*
We must connect probabilistic objectives (loss functions) to value-based objectives (human goals).

* **Outer Alignment**: Is the loss function we train on actually aligned with human goals?
* **Inner Alignment**: Given that loss, does the trained model's internal representation faithfully implement that goal, even off-distribution?

**Jagged Performance & Misalignment**
Current AI exhibits a "jagged frontier": it is unbelievably intelligent in some areas but fails at specific tasks (e.g., coding bugs). This suggests a lack of robust inner alignment.

**The "Goodhart's Law" of Biomarkers**
"When a biomarker is used to guide treatment decisions, it ceases to predict outcomes".
* *Example*: In pneumonia patients, high Blood Urea Nitrogen (BUN) predicts mortality. However, Creatinine levels show a "U-shaped" risk curve where low levels (usually healthy) showed higher risk—likely because those patients had low muscle mass (frailty).
* *Risk*: If a data-driven system learns that kidney failure treatments improve survival, it might learn to "put everyone into kidney failure" to minimize the predicted loss. This is accurate for the loss function, but misaligned with patient health.



### 3. Modularity
*Swappable, testable components.*
Interpretability allows us to connect component-level performance to system-level performance. If we can reverse-engineer models or build them with concept-based components, we can test and swap individual parts (like "modules") as better versions become available.

## Open Challenges & Takeaways

As Ilya Sutskever noted, "It's back to the age of research again, just with big computers".

* **Economic Impact**: Models show impressive eval performance but lack commensurate real-world economic impact.
* **Jaggedness**: Models repeat bugs and have uneven capabilities.
* **Emotional/Value Functions**: Humans use emotions as robust value functions to guide generalization. AI currently lacks this mechanism.
* **Data Walls**: Pre-training scales uniformly but is hitting data limits. RL consumes more compute but needs better efficiency via value functions.
* **Verifiable Rewards**: Scaling RL requires rewards that can be verified at scale.
* **Symbolic Reasoning**: Combining LLMs with symbolic reasoning and graphical models remains an open problem.
